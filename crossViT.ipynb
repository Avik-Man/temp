{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6ROvVHtRehY",
    "outputId": "d6afa73d-7b78-4243-ac6e-0cacffe13cf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: torchvision in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: einops in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: filelock in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (60.2.0)\n",
      "Requirement already satisfied: wheel in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.41.2)\n",
      "Requirement already satisfied: cmake in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from triton==2.0.0->torch) (3.29.3)\n",
      "Requirement already satisfied: lit in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from triton==2.0.0->torch) (18.1.4)\n",
      "Requirement already satisfied: numpy in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torchvision) (1.26.0)\n",
      "Requirement already satisfied: requests in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from torchvision) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from requests->torchvision) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from requests->torchvision) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision einops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uBHJzOv_RvCL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/cmatergpu/anaconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "  def __init__(self,dim,hidden_dim,dropout= 0.):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "        nn.LayerNorm(dim),\n",
    "        nn.Linear(dim,hidden_dim),\n",
    "        nn.GELU(),nn.Dropout(dropout),\n",
    "        nn.Linear(hidden_dim,dim),\n",
    "        nn.Dropout(dropout)\n",
    "    )\n",
    "    def forward(self,x):\n",
    "      return self.net(x)\n",
    "# Define data transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "batch_size = 32\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root='//run/media/cmatergpu/new_ssd/avikBCSE/ColorectalImage/ColorectalImage/train', transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "val_dataset = datasets.ImageFolder(root='/run/media/cmatergpu/new_ssd/avikBCSE/ColorectalImage/ColorectalImage/val', transform=val_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "test_dataset = datasets.ImageFolder(root='/run/media/cmatergpu/new_ssd/avikBCSE/ColorectalImage/ColorectalImage/test', transform=val_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qe3TXWvNVZig"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "# feedforward\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# attention\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, context = None, kv_include_self = False):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        x = self.norm(x)\n",
    "        context = default(context, x)\n",
    "\n",
    "        if kv_include_self:\n",
    "            context = torch.cat((x, context), dim = 1) # cross attention requires CLS token includes itself as key / value\n",
    "\n",
    "        qkv = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "# transformer encoder, for small and large patches\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return self.norm(x)\n",
    "\n",
    "# projecting CLS tokens, in the case that small and large patch tokens have different dimensions\n",
    "\n",
    "class ProjectInOut(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "        need_projection = dim_in != dim_out\n",
    "        self.project_in = nn.Linear(dim_in, dim_out) if need_projection else nn.Identity()\n",
    "        self.project_out = nn.Linear(dim_out, dim_in) if need_projection else nn.Identity()\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.project_in(x)\n",
    "        x = self.fn(x, *args, **kwargs)\n",
    "        x = self.project_out(x)\n",
    "        return x\n",
    "\n",
    "# cross attention transformer\n",
    "\n",
    "class CrossTransformer(nn.Module):\n",
    "    def __init__(self, sm_dim, lg_dim, depth, heads, dim_head, dropout):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                ProjectInOut(sm_dim, lg_dim, Attention(lg_dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                ProjectInOut(lg_dim, sm_dim, Attention(sm_dim, heads = heads, dim_head = dim_head, dropout = dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, sm_tokens, lg_tokens):\n",
    "        (sm_cls, sm_patch_tokens), (lg_cls, lg_patch_tokens) = map(lambda t: (t[:, :1], t[:, 1:]), (sm_tokens, lg_tokens))\n",
    "\n",
    "        for sm_attend_lg, lg_attend_sm in self.layers:\n",
    "            sm_cls = sm_attend_lg(sm_cls, context = lg_patch_tokens, kv_include_self = True) + sm_cls\n",
    "            lg_cls = lg_attend_sm(lg_cls, context = sm_patch_tokens, kv_include_self = True) + lg_cls\n",
    "\n",
    "        sm_tokens = torch.cat((sm_cls, sm_patch_tokens), dim = 1)\n",
    "        lg_tokens = torch.cat((lg_cls, lg_patch_tokens), dim = 1)\n",
    "        return sm_tokens, lg_tokens\n",
    "\n",
    "# multi-scale encoder\n",
    "\n",
    "class MultiScaleEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        depth,\n",
    "        sm_dim,\n",
    "        lg_dim,\n",
    "        sm_enc_params,\n",
    "        lg_enc_params,\n",
    "        cross_attn_heads,\n",
    "        cross_attn_depth,\n",
    "        cross_attn_dim_head = 64,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Transformer(dim = sm_dim, dropout = dropout, **sm_enc_params),\n",
    "                Transformer(dim = lg_dim, dropout = dropout, **lg_enc_params),\n",
    "                CrossTransformer(sm_dim = sm_dim, lg_dim = lg_dim, depth = cross_attn_depth, heads = cross_attn_heads, dim_head = cross_attn_dim_head, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, sm_tokens, lg_tokens):\n",
    "        for sm_enc, lg_enc, cross_attend in self.layers:\n",
    "            sm_tokens, lg_tokens = sm_enc(sm_tokens), lg_enc(lg_tokens)\n",
    "            sm_tokens, lg_tokens = cross_attend(sm_tokens, lg_tokens)\n",
    "\n",
    "        return sm_tokens, lg_tokens\n",
    "\n",
    "# patch-based image to token embedder\n",
    "\n",
    "class ImageEmbedder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        dropout = 0.,\n",
    "        channels = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        patch_dim = channels * patch_size ** 2\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim)\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "\n",
    "        return self.dropout(x)\n",
    "\n",
    "# cross ViT class\n",
    "\n",
    "class CrossViT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=224,\n",
    "        num_classes=9,\n",
    "        sm_dim=8,\n",
    "        lg_dim=16,\n",
    "        sm_patch_size = 8,\n",
    "        sm_enc_depth = 1,\n",
    "        sm_enc_heads = 8,\n",
    "        sm_enc_mlp_dim = 2048,\n",
    "        sm_enc_dim_head = 64,\n",
    "        lg_patch_size = 16,\n",
    "        lg_enc_depth = 4,\n",
    "        lg_enc_heads = 8,\n",
    "        lg_enc_mlp_dim = 2048,\n",
    "        lg_enc_dim_head = 64,\n",
    "        cross_attn_depth = 2,\n",
    "        cross_attn_heads = 8,\n",
    "        cross_attn_dim_head = 64,\n",
    "        depth = 3,\n",
    "        dropout = 0.1,\n",
    "        emb_dropout = 0.1,\n",
    "        channels = 3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sm_image_embedder = ImageEmbedder(dim = sm_dim, channels= channels, image_size = 224, patch_size = 8, dropout = emb_dropout)\n",
    "        self.lg_image_embedder = ImageEmbedder(dim = lg_dim, channels = channels, image_size = 224, patch_size = 16, dropout = emb_dropout)\n",
    "\n",
    "        self.multi_scale_encoder = MultiScaleEncoder(\n",
    "            depth = depth,\n",
    "            sm_dim = sm_dim,\n",
    "            lg_dim = lg_dim,\n",
    "            cross_attn_heads = cross_attn_heads,\n",
    "            cross_attn_dim_head = cross_attn_dim_head,\n",
    "            cross_attn_depth = cross_attn_depth,\n",
    "            sm_enc_params = dict(\n",
    "                depth = sm_enc_depth,\n",
    "                heads = sm_enc_heads,\n",
    "                mlp_dim = sm_enc_mlp_dim,\n",
    "                dim_head = sm_enc_dim_head\n",
    "            ),\n",
    "            lg_enc_params = dict(\n",
    "                depth = lg_enc_depth,\n",
    "                heads = lg_enc_heads,\n",
    "                mlp_dim = lg_enc_mlp_dim,\n",
    "                dim_head = lg_enc_dim_head\n",
    "            ),\n",
    "            dropout = dropout\n",
    "        )\n",
    "\n",
    "        self.sm_mlp_head = nn.Sequential(nn.LayerNorm(sm_dim), nn.Linear(sm_dim, num_classes))\n",
    "        self.lg_mlp_head = nn.Sequential(nn.LayerNorm(lg_dim), nn.Linear(lg_dim, num_classes))\n",
    "\n",
    "    def forward(self, img):\n",
    "        sm_tokens = self.sm_image_embedder(img)\n",
    "        lg_tokens = self.lg_image_embedder(img)\n",
    "\n",
    "        sm_tokens, lg_tokens = self.multi_scale_encoder(sm_tokens, lg_tokens)\n",
    "\n",
    "        sm_cls, lg_cls = map(lambda t: t[:, 0], (sm_tokens, lg_tokens))\n",
    "\n",
    "        sm_logits = self.sm_mlp_head(sm_cls)\n",
    "        lg_logits = self.lg_mlp_head(lg_cls)\n",
    "\n",
    "        return sm_logits + lg_logits\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 9\n",
    "\n",
    "# Instantiate the model\n",
    "model = CrossViT(num_classes=num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "AwjgNP6TSiEy",
    "outputId": "24c27764-f59f-489d-b5a3-220361bc798b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved\n",
      "Epoch 1/100, Train Loss: 1.6838, Train Accuracy: 39.36%, Val Loss: 1.8155, Val Accuracy: 30.66%\n",
      "model saved\n",
      "Epoch 2/100, Train Loss: 1.4800, Train Accuracy: 43.43%, Val Loss: 1.6604, Val Accuracy: 32.71%\n",
      "Epoch 3/100, Train Loss: 1.4274, Train Accuracy: 46.96%, Val Loss: 1.7071, Val Accuracy: 38.30%\n",
      "model saved\n",
      "Epoch 4/100, Train Loss: 1.4099, Train Accuracy: 46.81%, Val Loss: 1.5216, Val Accuracy: 40.73%\n",
      "Epoch 5/100, Train Loss: 1.3962, Train Accuracy: 47.29%, Val Loss: 1.5484, Val Accuracy: 42.03%\n",
      "model saved\n",
      "Epoch 6/100, Train Loss: 1.3187, Train Accuracy: 50.00%, Val Loss: 1.3545, Val Accuracy: 48.28%\n",
      "Epoch 7/100, Train Loss: 1.2522, Train Accuracy: 50.84%, Val Loss: 1.3623, Val Accuracy: 43.62%\n",
      "model saved\n",
      "Epoch 8/100, Train Loss: 1.1954, Train Accuracy: 52.71%, Val Loss: 1.2093, Val Accuracy: 48.18%\n",
      "model saved\n",
      "Epoch 9/100, Train Loss: 1.1682, Train Accuracy: 53.92%, Val Loss: 1.1587, Val Accuracy: 53.59%\n",
      "model saved\n",
      "Epoch 10/100, Train Loss: 1.0960, Train Accuracy: 60.55%, Val Loss: 1.0481, Val Accuracy: 56.38%\n",
      "model saved\n",
      "Epoch 11/100, Train Loss: 1.0163, Train Accuracy: 63.65%, Val Loss: 1.0322, Val Accuracy: 61.23%\n",
      "model saved\n",
      "Epoch 12/100, Train Loss: 0.9842, Train Accuracy: 65.52%, Val Loss: 1.0187, Val Accuracy: 62.44%\n",
      "model saved\n",
      "Epoch 13/100, Train Loss: 0.9737, Train Accuracy: 64.98%, Val Loss: 0.9829, Val Accuracy: 62.81%\n",
      "model saved\n",
      "Epoch 14/100, Train Loss: 0.9605, Train Accuracy: 65.64%, Val Loss: 0.9636, Val Accuracy: 63.28%\n",
      "model saved\n",
      "Epoch 15/100, Train Loss: 0.9564, Train Accuracy: 65.85%, Val Loss: 0.9391, Val Accuracy: 64.21%\n",
      "Epoch 16/100, Train Loss: 0.9480, Train Accuracy: 66.46%, Val Loss: 0.9751, Val Accuracy: 64.12%\n",
      "model saved\n",
      "Epoch 17/100, Train Loss: 0.9343, Train Accuracy: 66.79%, Val Loss: 0.9324, Val Accuracy: 64.40%\n",
      "model saved\n",
      "Epoch 18/100, Train Loss: 0.9332, Train Accuracy: 65.85%, Val Loss: 0.9185, Val Accuracy: 65.05%\n",
      "Epoch 19/100, Train Loss: 0.9207, Train Accuracy: 67.63%, Val Loss: 0.9230, Val Accuracy: 65.24%\n",
      "model saved\n",
      "Epoch 20/100, Train Loss: 0.9123, Train Accuracy: 67.21%, Val Loss: 0.9111, Val Accuracy: 65.33%\n",
      "model saved\n",
      "Epoch 21/100, Train Loss: 0.9027, Train Accuracy: 67.42%, Val Loss: 0.9065, Val Accuracy: 65.24%\n",
      "model saved\n",
      "Epoch 22/100, Train Loss: 0.9136, Train Accuracy: 67.48%, Val Loss: 0.9038, Val Accuracy: 64.86%\n",
      "model saved\n",
      "Epoch 23/100, Train Loss: 0.8951, Train Accuracy: 68.20%, Val Loss: 0.8997, Val Accuracy: 64.77%\n",
      "Epoch 24/100, Train Loss: 0.8891, Train Accuracy: 68.32%, Val Loss: 0.9007, Val Accuracy: 65.52%\n",
      "model saved\n",
      "Epoch 25/100, Train Loss: 0.9032, Train Accuracy: 67.84%, Val Loss: 0.8988, Val Accuracy: 65.33%\n",
      "model saved\n",
      "Epoch 26/100, Train Loss: 0.8916, Train Accuracy: 67.87%, Val Loss: 0.8922, Val Accuracy: 65.24%\n",
      "model saved\n",
      "Epoch 27/100, Train Loss: 0.9114, Train Accuracy: 67.27%, Val Loss: 0.8908, Val Accuracy: 65.42%\n",
      "Epoch 28/100, Train Loss: 0.9001, Train Accuracy: 67.72%, Val Loss: 0.8923, Val Accuracy: 65.33%\n",
      "Epoch 29/100, Train Loss: 0.8903, Train Accuracy: 68.99%, Val Loss: 0.8910, Val Accuracy: 65.52%\n",
      "Epoch 30/100, Train Loss: 0.9068, Train Accuracy: 68.05%, Val Loss: 0.8973, Val Accuracy: 65.89%\n",
      "Epoch 31/100, Train Loss: 0.9029, Train Accuracy: 67.93%, Val Loss: 0.8968, Val Accuracy: 65.80%\n",
      "Epoch 32/100, Train Loss: 0.8930, Train Accuracy: 67.66%, Val Loss: 0.8956, Val Accuracy: 65.98%\n",
      "Epoch 33/100, Train Loss: 0.8912, Train Accuracy: 68.87%, Val Loss: 0.8946, Val Accuracy: 65.89%\n",
      "Epoch 34/100, Train Loss: 0.8849, Train Accuracy: 68.08%, Val Loss: 0.8934, Val Accuracy: 66.08%\n",
      "Epoch 35/100, Train Loss: 0.8829, Train Accuracy: 68.63%, Val Loss: 0.8929, Val Accuracy: 66.08%\n",
      "Epoch 36/100, Train Loss: 0.8966, Train Accuracy: 68.26%, Val Loss: 0.8917, Val Accuracy: 65.89%\n",
      "Epoch 37/100, Train Loss: 0.8844, Train Accuracy: 68.20%, Val Loss: 0.8919, Val Accuracy: 65.89%\n",
      "Epoch 38/100, Train Loss: 0.8925, Train Accuracy: 68.02%, Val Loss: 0.8928, Val Accuracy: 65.98%\n",
      "Epoch 39/100, Train Loss: 0.8894, Train Accuracy: 68.35%, Val Loss: 0.8924, Val Accuracy: 65.98%\n",
      "Epoch 40/100, Train Loss: 0.9016, Train Accuracy: 68.32%, Val Loss: 0.8918, Val Accuracy: 65.89%\n",
      "Epoch 41/100, Train Loss: 0.8832, Train Accuracy: 68.35%, Val Loss: 0.8917, Val Accuracy: 65.89%\n",
      "Epoch 42/100, Train Loss: 0.8959, Train Accuracy: 68.38%, Val Loss: 0.8916, Val Accuracy: 65.89%\n",
      "Epoch 43/100, Train Loss: 0.8992, Train Accuracy: 67.57%, Val Loss: 0.8916, Val Accuracy: 65.89%\n",
      "Epoch 44/100, Train Loss: 0.8758, Train Accuracy: 68.63%, Val Loss: 0.8915, Val Accuracy: 65.89%\n",
      "Epoch 45/100, Train Loss: 0.8923, Train Accuracy: 68.38%, Val Loss: 0.8916, Val Accuracy: 65.89%\n",
      "Epoch 46/100, Train Loss: 0.8924, Train Accuracy: 68.17%, Val Loss: 0.8917, Val Accuracy: 65.89%\n",
      "Epoch 47/100, Train Loss: 0.9018, Train Accuracy: 68.02%, Val Loss: 0.8917, Val Accuracy: 65.98%\n",
      "Epoch 48/100, Train Loss: 0.9063, Train Accuracy: 67.87%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 49/100, Train Loss: 0.8947, Train Accuracy: 68.14%, Val Loss: 0.8919, Val Accuracy: 65.98%\n",
      "Epoch 50/100, Train Loss: 0.8882, Train Accuracy: 67.96%, Val Loss: 0.8919, Val Accuracy: 65.98%\n",
      "Epoch 51/100, Train Loss: 0.8889, Train Accuracy: 68.26%, Val Loss: 0.8919, Val Accuracy: 65.98%\n",
      "Epoch 52/100, Train Loss: 0.8876, Train Accuracy: 67.60%, Val Loss: 0.8919, Val Accuracy: 65.98%\n",
      "Epoch 53/100, Train Loss: 0.8870, Train Accuracy: 68.54%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 54/100, Train Loss: 0.8888, Train Accuracy: 68.14%, Val Loss: 0.8919, Val Accuracy: 65.98%\n",
      "Epoch 55/100, Train Loss: 0.8911, Train Accuracy: 67.66%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 56/100, Train Loss: 0.8991, Train Accuracy: 67.63%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 57/100, Train Loss: 0.8900, Train Accuracy: 67.84%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 58/100, Train Loss: 0.8845, Train Accuracy: 68.29%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 59/100, Train Loss: 0.9029, Train Accuracy: 67.39%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 60/100, Train Loss: 0.9038, Train Accuracy: 67.72%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 61/100, Train Loss: 0.8916, Train Accuracy: 68.14%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 62/100, Train Loss: 0.8711, Train Accuracy: 68.81%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 63/100, Train Loss: 0.8718, Train Accuracy: 68.60%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 64/100, Train Loss: 0.8929, Train Accuracy: 67.81%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 65/100, Train Loss: 0.8860, Train Accuracy: 68.14%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 66/100, Train Loss: 0.8854, Train Accuracy: 67.78%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 67/100, Train Loss: 0.8960, Train Accuracy: 68.11%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 68/100, Train Loss: 0.8861, Train Accuracy: 68.17%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 69/100, Train Loss: 0.8967, Train Accuracy: 68.11%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 70/100, Train Loss: 0.8932, Train Accuracy: 68.02%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 71/100, Train Loss: 0.8871, Train Accuracy: 68.17%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 72/100, Train Loss: 0.8841, Train Accuracy: 68.75%, Val Loss: 0.8918, Val Accuracy: 65.98%\n",
      "Epoch 73/100, Train Loss: 0.8880, Train Accuracy: 67.96%, Val Loss: 0.8918, Val Accuracy: 65.98%\n"
     ]
    }
   ],
   "source": [
    "# Define the number of classes\n",
    "num_classes = 9\n",
    "\n",
    "# Instantiate the model\n",
    "model = CrossViT().cuda()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Training loop with plotting\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_loss_min = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= train_total\n",
    "    train_accuracy = 100. * train_correct / train_total\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss /= val_total\n",
    "    if(val_loss < val_loss_min):\n",
    "        val_loss_min = val_loss \n",
    "        torch.save(model,'./model.pt')\n",
    "        print('model saved')\n",
    "    val_accuracy = 100. * val_correct / val_total\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Jda3IewgMju"
   },
   "outputs": [],
   "source": [
    "# Plotting accuracy and loss\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, train_losses, label='Training Loss')\n",
    "plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
